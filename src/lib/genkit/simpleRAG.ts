/**
 * üèóÔ∏è Simple RAG Flow - Direct Implementation
 * 
 * Simplified RAG without Genkit dependency
 * Uses Google AI API directly
 */

import { GoogleGenerativeAI } from '@google/generative-ai';

const GOOGLE_AI_API_KEY = 'AIzaSyDQT4sxlCRVxm0xqvfzaBIobv-3y8KfV-k';

interface RAGContext {
  text: string;
  title: string;
  quizId?: string;
  score: number;
}

interface RAGResponse {
  answer: string;
  citations: Array<{ title: string; quizId?: string }>;
  usedChunks: number;
  processingTime: number;
  tokensUsed: { input: number; output: number };
}

/**
 * Generate embedding using Google AI
 */
async function generateEmbedding(text: string): Promise<number[]> {
  const genAI = new GoogleGenerativeAI(GOOGLE_AI_API_KEY);
  const model = genAI.getGenerativeModel({ model: 'text-embedding-004' });
  
  const result = await model.embedContent(text);
  return result.embedding.values;
}

/**
 * Calculate cosine similarity between two vectors
 */
function cosineSimilarity(a: number[], b: number[]): number {
  let dotProduct = 0;
  let normA = 0;
  let normB = 0;
  
  for (let i = 0; i < a.length; i++) {
    dotProduct += a[i] * b[i];
    normA += a[i] * a[i];
    normB += b[i] * b[i];
  }
  
  return dotProduct / (Math.sqrt(normA) * Math.sqrt(normB));
}

/**
 * Simple vector search in local storage index
 */
async function vectorSearch(
  question: string,
  topK: number = 4
): Promise<RAGContext[]> {
  // Get index from localStorage
  const indexStr = localStorage.getItem('vector-index');
  if (!indexStr) {
    throw new Error('Vector index not found. Please build index first.');
  }
  
  const index = JSON.parse(indexStr);
  
  // Generate question embedding
  const questionEmbedding = await generateEmbedding(question);
  
  // Calculate similarities
  const results = index.chunks.map((chunk: any) => ({
    text: chunk.text,
    title: chunk.title,
    quizId: chunk.quizId,
    score: cosineSimilarity(questionEmbedding, chunk.embedding),
  }));
  
  // Sort by similarity and take top K
  results.sort((a: any, b: any) => b.score - a.score);
  return results.slice(0, topK);
}

/**
 * Generate answer using retrieved context
 */
async function generateAnswer(
  question: string,
  contexts: RAGContext[],
  targetLang: string = 'vi'
): Promise<{ answer: string; tokensUsed: { input: number; output: number } }> {
  const genAI = new GoogleGenerativeAI(GOOGLE_AI_API_KEY);
  const model = genAI.getGenerativeModel({ model: 'gemini-2.0-flash-exp' });
  
  // Build context string
  const contextStr = contexts
    .map((ctx, i) => `[${i + 1}] ${ctx.title}\n${ctx.text}`)
    .join('\n\n');
  
  const prompt = `B·∫°n l√† AI Learning Assistant - tr·ª£ l√Ω h·ªçc t·∫≠p th√¥ng minh cho sinh vi√™n.

**NHI·ªÜM V·ª§:**
D·ª±a v√†o c√°c th√¥ng tin t·ª´ quiz/t√†i li·ªáu b√™n d∆∞·ªõi, h√£y tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa sinh vi√™n m·ªôt c√°ch chi ti·∫øt v√† d·ªÖ hi·ªÉu.

**PHONG C√ÅCH:**
- Th√¢n thi·ªán, nhi·ªát t√¨nh nh∆∞ m·ªôt ng∆∞·ªùi b·∫°n h·ªçc
- Gi·∫£i th√≠ch t·ª´ c∆° b·∫£n ƒë·∫øn n√¢ng cao
- S·ª≠ d·ª•ng v√≠ d·ª• th·ª±c t·∫ø ƒë·ªÉ minh h·ªça
- D√πng emoji ƒë·ªÉ t·∫°o kh√¥ng kh√≠ tho·∫£i m√°i
- Khuy·∫øn kh√≠ch suy nghƒ© v√† h·ªçc h·ªèi

**ƒê·ªäNH D·∫†NG TR·∫¢ L·ªúI:**
üìö **Gi·∫£i Th√≠ch:** [Gi·∫£i th√≠ch chi ti·∫øt]
üí° **V√≠ D·ª•:** [V√≠ d·ª• th·ª±c t·∫ø n·∫øu c√≥]
‚úÖ **Ghi Nh·ªõ:** [M·∫πo ghi nh·ªõ n·∫øu ph√π h·ª£p]
üéØ **Luy·ªán T·∫≠p:** [G·ª£i √Ω quiz li√™n quan]

Lu√¥n tr√≠ch d·∫´n ngu·ªìn b·∫±ng [1], [2], etc. khi s·ª≠ d·ª•ng th√¥ng tin t·ª´ context.

---

**CONTEXT T·ª™ QUIZ/T√ÄI LI·ªÜU:**

${contextStr}

---

**C√ÇU H·ªéI C·ª¶A SINH VI√äN:**
${question}

**TR·∫¢ L·ªúI (b·∫±ng ${targetLang === 'vi' ? 'Ti·∫øng Vi·ªát' : 'English'}):**`;

  const result = await model.generateContent(prompt);
  const response = result.response;
  const answer = response.text();
  
  // Estimate tokens (rough approximation)
  const inputTokens = Math.ceil(prompt.length / 4);
  const outputTokens = Math.ceil(answer.length / 4);
  
  return {
    answer,
    tokensUsed: {
      input: inputTokens,
      output: outputTokens,
    },
  };
}

/**
 * Main RAG flow
 */
export async function askQuestion(params: {
  question: string;
  topK?: number;
  targetLang?: string;
}): Promise<RAGResponse> {
  const startTime = Date.now();
  
  const { question, topK = 4, targetLang = 'vi' } = params;
  
  try {
    // 1. Retrieve relevant contexts
    const contexts = await vectorSearch(question, topK);
    
    // 2. Generate answer
    const { answer, tokensUsed } = await generateAnswer(
      question,
      contexts,
      targetLang
    );
    
    // 3. Extract citations
    const citations = contexts.map(ctx => ({
      title: ctx.title,
      quizId: ctx.quizId,
    }));
    
    return {
      answer,
      citations,
      usedChunks: contexts.length,
      processingTime: Date.now() - startTime,
      tokensUsed,
    };
    
  } catch (error) {
    console.error('RAG Error:', error);
    throw error;
  }
}
